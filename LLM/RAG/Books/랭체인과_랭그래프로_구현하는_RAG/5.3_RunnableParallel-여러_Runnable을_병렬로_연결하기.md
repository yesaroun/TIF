RunnableParallel-여러 Runnable을 병렬로 연결하기

LCEL을 구현하다 보면 Runnable을 병렬로 연결하고 싶을때가 있다. 예를 들어 사용자가 입력한 주제에 대해 LLM에게 낙관적인 의견과 비관적인 의견을 생성하게 해 보겠다.

```python
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

model = ChatOpenAI(model="gpt-4o-mini", temperature=0)
output_parser = StrOutputParser()
```

낙관적인 의견을 생성하는 Chain을 구현한다.

```python
optimistic_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "당신은 낙관주의자입니다. 사용자의 입력에 대해 낙관적인 의견을 제공하세요."),
        ("human", "{topic}"),
    ]
)
optimistic_chain = optimistic_prompt | model | output_parser
```

비관적인 의견을 생성하는 Chain 구현

```python
pessimistic_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "당신은 비관주의자입니다. 사용자의 입력에 대해 비관적인 의견을 제공하세요."),A
        ("human", "{topic}"),
    ]
)
pessimistic_chain = pessimistic_prompt | model | output_parser
```

`RunnableParallel`을 사용하여 낙관적인 의견을 생성하는 Chain과 비관적인 의견을 생성하는 Chain을 병렬로 연결한 Chain을 만들고 실행하겠다.

```python
import pprint
from langchain_core.runnables import RunnableParallel

parallel_chain = RunnableParallel(
    {
        "optimistic_opinion": optimistic_chain,
        "pessimistic_opinion": pessimistic_chain,
    }
)

output = parallel_chain.invoke({"topic": "생성 AI의 진화에 관해"})
pprint.pprint(output)
```

```
{'optimistic_opinion': '생성 AI의 진화는 정말 ....', 'pessimistic_opinion': '생성 AI의 진화는 분명 ...'}
```

낙관적인 의견과 비관적인 의견이 dict 형태로 출력되었다. 이처럼 여러 Runnable을 병렬로 연결하여 실행할 수 있는 것이 `RunnableParallel`이다.

위 예지의 `optimistic_chain`과 `pessimistic_chain`은 동시에 실행되므로 순차적으로 실행하는 것보다 짧은 시간에 전체 처리가 완료된다.

RunnableParallel은 본질적으로 키가 str이고 값이 Runnable인 dict이다.

# RunnableParallel의 출력을 Runnable의 입력으로 연결하기

RunnableParallel도 Runnable의 일종이므로 Runnable과 `|`로 연결할 수 있다. 낙관적 의견과 비관적 의견을 제시한 후 객관적으로 요약하는 Chain을 구성해보겠다.

낙관적인 의견과 비관적인 의견을 요약하는 프롬프트(`synthesize_prompt`)를 준비한다.

```python
synthesize_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "당신은 객관적 AI입니다. 두 가지 의견을 종합하세요."),
        ("human", "낙관적 의견: {optimistic_opinion}\n비관적 의견: {pessimistic_opinion}"),
    ]
)
```

낙관적 의견을 생성하는 Chain과 비관적 의견을 생성하는 Chain을 병렬로 연결한 RunnableParallel을 `synthesize_prompt`, `model`, `output_parser`에 연결해 실행한다.

```python
synthesize_chain = (
    RunnableParallel(
        {
            "optimistic_opinion": optimistic_chain,
            "pessimistic_opinion": pessimistic_chain,
        }
    )
    | synthesize_prompt 
    | model
    | output_parser
)

output = synthesize_chain.invoke({"topic": "생성 AI의 진화에 관해"})
print(output)
```

RunnableParallel의 두 Chain으로 생성한 의견을 요약하는데 성공했다.

# RunnableParallel 자동 변환

앞에서 Runnable과 함수를 `|`로 연결하면 함수가 자동으로 RunnableLambda로 변환되었다. 마찬가지로, 키가 str이고 값이 Runnable인 dict는 RunnableParallel로 자동 변환된다.

```python
synthesize_chain = (
    {
        "optimistic_opinion": optimistic_chain,
        "pessimistic_opinion": pessimistic_chain,
    }  # RunnableParallel로 자동 변환됨
    | synthesize_prompt
    | model
    | output_parser
)
```

# RunnableLambda와의 조합 - itemgetter를 사용한 예시

itemgetter는 Python 표준 라이브러리에서 제공하는 함수이다. itemgetter를 사용하면 dict 등에서 값을 추출하는 함수를 쉽게 만들 수 있다.

예를 들어 `{"topic": "생성 AI의 진화에 관해"}` 라는 dict에서 itemgetter("topic")을 사용해 토픽을 추출하는 예는 다음과 같다.

```python
from operator import itemgetter

topic_getter = itemgetter("topic")
topic = topic_getter({"topic": "생성 AI의 진화에 관해"})
print(topic)
# 생성 AI의 진화에 관해
```

LCEL에서는 이 itemgetter를 유용하게 사용하는 경우가 많다. 예를 들어 다음 코드는 `{"topic": "생성 AI의 진화에 관해"}`에서 itemgetter("topic")으로 값을 추출해 ChatPromptTemplate의 {topic} 부분에 채워 넣는다.

```python
from operator import itemgetter

synthesize_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "당신은 객관적 AI입니다. {topic}에 대한 두 가지 의견을 종합하세요.",
        ),
        (
            "human",
            "낙관적 의견: {optimistic_opinion}\n비관적 의견: {pessimistic_opinion}",
        ),
    ]
)

synthesize_chain = (
    {
        "optimistic_opinion": optimistic_chain,
        "pessimistic_opinion": pessimistic_chain,
        "topic": itemgetter("topic"),  # RunnableLambda(itemgetter("topic"))로 자동 변환됨
    }
    | synthesize_prompt
    | model
    | output_parser
)

output = synthesize_chain.invoke({"topic": "생성 AI의 진화에 관해"})
print(output)
```

