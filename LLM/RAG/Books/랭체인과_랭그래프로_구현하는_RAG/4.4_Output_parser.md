
LLM을 특정 형식으로 출력하도록 하고 그 출력을 프로그램적으로 다루고 싶을때 사용 할 수 있는 것이 `Output parser` 이다.

# PydanticOutputParser를 사용한 Python 객체 변환

`PydanticOutputParser`를 사용하면 LLM 출력을 Python 객체로 변환할 수 있다.

여기서는 Output parser 개념을 이해하기 위해 `PydanticOutputParser`를 설명하나 실제 프로젝트에서는 `with_structured_output`을 사용하는 것을 권장

우선 재료 목록(ingredients)와 순서(steps)를 필드로 하는 Recipe 클래스를 Pydantic의 모델로 정의

```python
from pydantic import BaseModel, Field

class Recipe(BaseModel):
    ingredients: list[str] = Field(description="ingredients of the dish")
    steps: list[str] = Field(description="steps to make the dish")
```

```python
from langchain_core.output_parsers import PydanticOutputParser

output_parser = PydanticOutputParser(pydantic_object=Recipe)

format_instructions = output_parser.get_formant_instructions()
print(format_instructions)
"""
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, fore the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
...

Here is the output schema:
'''
{"properties": {"ingredients": {"description": "ingredients of the dish", "items": {"type": "string"}, 
...
'''
"""
```

출력은 이런 JSON 형식으로 해주세요 라는 내용이다. 이 `format_instructions`를 프롬프트에 포함시킴으로써 LLM이 형식에 맞는 응답을 반환하도록 한다.

이어서 `format_instructions`를 사용한 `ChatPromptTemplate`를 생성한다.

```python
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "사용자가 입력한 요리의 레시피를 생각해 주세요.\n\n"
            "{format_instructions}",
        ),
        ("human", "{dish}"),
    ]
)

prompt_with_format_instructions = prompt.partial(
    format_instructions=format_instructions
)
```

`prompt.partial` 이라는 부분에서 프롬프트 일부를 채우고 있다. 이 `ChatPromptTemplate`에 예시로 입력을 제공해 보겠다.

```python
prompt_value = prompt_with_format_instructions.invoke({"dish": "카레"})
print("=== role: system ===")
print(prompt_value.messages[0].content)
print("=== role: user ===")
print(prompt_value.messages[1].content)
```

```text
=== role: system ===
사용자가 입력한 요리의 레시피를 생각해 주세요.

The output should be formatted as a JSON ...

An an example, for the schema {"properties": {"foo": ...

Here is the output schema:
'''
{"properties": {"ingredients": ...
'''

=== role: user ===
카레
```

Recipe 클래스 정의를 바탕으로 출력 형식을 지정하는 프롬프트가 자동으로 포함됐다. 이 텍스트를 입력으로 LLM을 실행해 보겠다.

```python
from langchain_openai import ChatOpenAI

model = ChatOpenAI(model="gpt-4o-mini", temperature=0)

ai_message = model.invoke(prompt_value)
print(ai_message.content)
```

```text
{
    "ingredients": [
        "닭고기 500g",
        ...
    ],
    "steps": [
        "닭고기를 한 입 크기로 ...",
        ...
    ]
}
```

이 응답을 Pydantic 모델의 인스턴스로 변환하고 싶으면 변환 처리를 `PydanticOutputParser`를 사용하면 간단하다.

```python
recipe = output_parser.invoke(ai_message)
print(type(recipe))
print(recipe)
```

Output parser의 핵심은 다음 두 가지 이다.

- Recipe 클래스 정의를 바탕으로, 출력 형식을 지정하는 문자열이 자동으로 만들어졌다.
- LLM의 출력을 쉽게 Recipe 클래스의 인스턴스로 변환할 수 있었다.

이처럼 매우 편리한 Output parser지만, LLM이 불완전한 JSON을 반환하면 오류가 발생할 수 있다. 안정적으로 출력하게 하려면 Chat Completions API의 JSON 모드 같은 기능을 사요하거나 Function calling을 응용하는 것이 유용하다.

# StrOutputParser

StrOutputParser는 LLM 출력을 텍스트로 변환하는데 사용한다. 예를 들어 ChatOpenAI를 invoke하면 AIMessage가 반환된다. AIMessage에 StrOutputParser를 invoke하면 텍스트를 추출 할 수 있다.

```python
from langchain_core.messages import AIMessage
from langchain_core.output_parsers import StrOutputParser

output_parser = StrOutputParser()

ai_message = AIMessage(content="안녕하세요. 저는 AI 어시스턴트입니다.")
ai_message = output_parser.invoke(ai_message)
print(type(ai_message))
# <class 'str'>
print(ai_message)
# 안녕하세요. 저는 AI 어시스턴트입니다.
```

이 샘플 코드만 보면 '굳이 StrOutputParser를 사용하지 않고, ai_message.content 라고 작성해 텍스트를 추출하면 되지 않을까'라고 생각할 수도 있지만, 다음 절에서 설명할 LangChain Expression Language(LCEL)의 구성 요소로서 중요한 역할을 한다. 이 부분을 배우면 왜 StrOutputParser를 사용하는지 이해할 수 있다.


