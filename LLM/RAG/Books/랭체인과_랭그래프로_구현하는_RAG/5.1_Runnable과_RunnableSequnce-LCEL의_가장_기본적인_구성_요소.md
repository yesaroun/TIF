Runnable과 RunnableSequence-LCEL의 가장 기본적인 구성 요소(5.1)

LCEL의 가장 기본적인 구현은 Prompt template, Chat model, Output parser의 세 가지를 연결하는 것

```python
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "사용자가 입력한 요리의 레시피를 생각해 주세요."),
        ("human", "{dish}"),
    ]
)      

model = ChatOpenAI(model="gpt-4o-mini", temperature=0)

output_parser = StrOutputParser()
```

prompt, model, `output_parser` 를 순서대로 invoke 하는 코드는 다음과 같다.

```python
prompt_value = prompt.invoke({"dish": "카레"})
ai_message = model.invoke(prompt_value)
output = output_parser.invoke(ai_message)

print(output)
```

이렇게 순서대로 실행할 수 있지만, LCEL에서 이들을 `|`로 연결한 후 실행한다. 사실, ChatPromptTemplate, ChatOpenAI, StrOutputParser 는 모두 LangChain의 `Runnable` 이라는 추상 기본 클래스를 상속받고 있다. Runnable 을 `|` 로 연결하면 `RunnableSequence`가 된다. RunnableSequence도 Runnable의 일종이다.

RunnableSequence를 invoke하면 연결된 Runnable이 순서대로 invoke 된다.

```python
chain = prompt | model | output_parser
output = chain.invoke({"dish": "카레"})
```

# Runnable의 실행 방법-invoke, stream, batch

Runnable의 실행 방법으로는 invoke 외에도 stream, batch가 있다.  

Runnable을 스트리밍으로 실행하려면 stream 메서드를 사용한다.

```python
chain = prompt | model | output_parser

for chunk in chain.stream({"dish": "카레"}):
    print(chain, end="", flush=True)
```

또한, batch 메서드를 사용하면 여러 입력을 한꺼번에 처리할 수 있다.

```python
chain = prompt | model | output_parser

outputs = chain.batch([{"dish": "카레"}, {"dish": "우동"}])
print(outputs)
```

# LCEL은 어떻게 구현됐는가

LCEL은 LangChain의 각종 모듈이 상속 받는 Runnable 클래스를 통해 구현된다. LangChain(langchain-core)의 소스 코드에서 Runnable은 추상 기본 클래스(ABC)로 정의돼 있다.

```python
class Runnable(Generic[Input, Output], ABC):
    def __or__(self,
            other: Union[Runnable[Any, Other],
                    Callable[[Any], Other],
                    Callable[[Iterator[Any]], Iterator[Other]],
                    Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]],
                ],
        ) -> RunnableSerializable[Input, Other]:
        """Compose this runnable with another object to create a RunnableSequence."""
        return RunnableSequence(first=self, last=coerce_to_runnable(other))
    
    def __ror__(self,
        other: Union[Runnable[Other, Any],
                    Callable[[Other], Any],
                    Callable[[Iterator[Other]], Iterator[Any]],
                    Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]],
                ],
        ) -> RunnableSerializable[Other, Output]:
        """Compose this runnable with another object to create a RunnableSequence."""
        return RunnableSequence(first=core_to_runnable(other), last=self)
```

Python에서는 `__or__`나 `__ror__`를 통해 `|` 연산자를 오버로드할 수 있다. 그래서 `chain = prompt | model` 과 같은 표기법이 가능하다.

# LCEL의 '|'로 다양한 Runnable 연결하기

`|`를 사용하면 Runnable과 Runnable을 연결할 수 있다. Runnable을 연결한 chain도 Runnable이므로, chain과 chain도 `|`로 연결할 수 있다. 예를 들어 Zero-shot CoT로 단계적으로 생각한 다음, 그 결과에서 결론만 추출하는 코드를 작성해 보겠다.

```python
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

model = ChatOpenAI(model="gpt-4o-mini", temperature=0)

output_parser = StrOutputParser()
```

첫 번째로, Zero-shot CoT로 단계적으로 생각하게 하는 Chain을 만든다.

```python
cot_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "사용자의 질문에 단계적으로 답변하세요."),
        ("human", "{question}"),
    ]
)

cot_chain = cot_prompt | model | output_parser
```

두 번째로, 단계적으로 생각한 답변에서 결론을 추출하는 Chain을 만듭니다.

```python
summarize_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "단계적으로 생각한 답변에서 결론만 추출하세요."),
        ("human", "{text}"),
    ]
)

summarize_chain = summarize_prompt | model | output_parser
```

두 개의 Chain을 연결한 Chain을 만들고 실행해 본다.

```python
cot_summarize_chain = cot_chain | summarize_chain
output = cot_summarize_chain.invoke({"question": "10 + 2 * 3"})
print(output)  # 16
```

LLM을 2번 호출함으로써 Zero-shot CoT를 사용해 답변의 정확도를 높이면서도 최종적으로는 간결한 출력을 얻을 수 있었다.

LLM 애플리케이션에서는 복잡한 태스크를 한 번의 LLM 호출로 해결하려고 하면 프롬프트 작성이나 개선이 어려워지는 경우가 많다. 여러 프롬프트를 여러 번 LLM을 호출하는 방식을 취하면 태스크를 쉽게 해결할 수 있는 경우가 많다.

참고로 `cot_chain`과 `summarize_chain`을 `|`로 연결하여 실행할 수 있는 것은 `cot_chain`의 출력 타입과 `summarize_chain`의 입력 타입의 일관성이 유지되기 때문이다.

**Runnable을 `|`로 연결할 때는 출력 타입과 입력 타입의 일관성에 주의해야 합니다**

