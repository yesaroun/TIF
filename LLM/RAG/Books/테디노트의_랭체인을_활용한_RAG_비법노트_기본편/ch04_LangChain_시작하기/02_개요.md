### 1\. 개발 환경 설정: API 키와 LangSmith 연동

#### **API 키 관리 (`.env` 파일)**

`python-dotenv` 라이브러리를 사용하면, `.env` 라는 파일에 API 키를 저장하고 환경 변수로 불러와 안전하게 사용할 수 있다.

```python
# API KEY를 환경변수로 관리하기 위한 설정 파일
from dotenv import load_dotenv

# API KEY 정보로드
load_dotenv()
```

#### **LangSmith로 개발 과정 추적하기**

LangChain에서 제공하는 **LangSmith**는 LLM 애플리케이션의 모든 호출과 응답을 기록하고 시각화해주는 강력한 도구다. 복잡한 체인(Chain)이나 에이전트(Agent)를 개발할 때, 각 단계에서 어떤 일이 일어나는지 명확하게 파악할 수 있어 디버깅에 큰 도움이 된다.

```python
# LangSmith 추적을 설정한다. https://smith.langchain.com
# .env 파일에 LANGCHAIN_API_KEY를 입력한다.
# !pip install -qU langchain-teddynote
from langchain_teddynote import logging

# 프로젝트 이름을 입력한다.
logging.langsmith("CH01-Basic")
```

-----

### 2\. ChatOpenAI

`ChatOpenAI`는 LangChain에서 OpenAI의 챗봇 모델을 사용하기 위한 핵심 클래스다. 객체를 생성할 때 몇 가지 중요한 옵션을 설정하여 모델의 답변을 제어할 수 있다.

  * **`temperature`**: 모델의 **창의성**을 조절하는 값(0\~2)이다. 낮을수록(예: 0.1) 일관되고 사실에 기반한 답변을, 높을수록(예: 0.8) 더 다양하고 창의적인 답변을 생성한다.
  * **`max_tokens`**: 답변으로 생성될 **최대 토큰 수**를 제한한다. 토큰은 단어나 문장의 일부로, 비용과 답변 길이에 직접적인 영향을 준다.
  * **`model_name`**: 사용할 OpenAI 모델의 이름을 지정한다. (예: `gpt-4.1-nano`, `gpt-4.1-mini` 등)

```python
from langchain_openai import ChatOpenAI

# LLM 객체 생성
llm = ChatOpenAI(
    temperature=0.1,  # 창의성 (0.0 ~ 2.0)
    model_name="gpt-4.1-nano",  # 모델명
)

# 질의내용
question = "대한민국의 수도는 어디인가요?"

# LLM에게 질문하고 답변 받기
response = llm.invoke(question)
print(response.content)
```

#### **답변의 구조**

`invoke` 메소드의 반환값은 단순한 문자열이 아니라, `AIMessage`라는 객체다. 이 객체 안에는 다양한 정보가 담겨 있다.

  * **`content`**: 우리가 흔히 보는 모델의 답변 내용이다.
  * **`response_metadata`**: 토큰 사용량, 응답 시간, API 요청 ID 등 부가적인 메타데이터가 포함되어 있어 비용 계산이나 로깅에 유용하다.

-----

### 3\. LogProbs와 스트리밍

#### **LogProbs**

`logprobs` 옵션을 활성화하면, 모델이 각 토큰을 생성할 때 얼마나 확신했는지를 **로그 확률** 값으로 받아볼 수 있다. 이는 모델 답변의 신뢰도를 평가하거나, 특정 답변이 나온 이유를 분석할 때 유용하다.

```python
# logprobs=True 옵션을 추가하여 객체 생성
llm_with_logprob = ChatOpenAI(
    temperature=0.1,
    max_tokens=2048,
    model_name="gpt-4.1-nano",
).bind(logprobs=True)

# 질의
response = llm_with_logprob.invoke("대한민국의 수도는 어디인가요?")

# 메타데이터에서 logprobs 정보 확인
print(response.response_metadata)
```

#### **스트리밍**

LLM이 긴 답변을 생성할 때, 전체 답변이 완료될 때까지 기다리는 것은 사용자 경험에 좋지 않다. `stream` 메소드를 사용하면, 마치 채팅 앱처럼 답변이 생성되는 대로 토큰을 하나씩 실시간으로 받아 처리할 수 있다.

```python
from langchain_teddynote.messages import stream_response

# 스트림 방식으로 질의
answer_stream = llm.stream("대한민국의 아름다운 관광지 10곳과 주소를 알려주세요!")

# 스트리밍 응답을 실시간으로 출력
stream_response(answer_stream)
```

-----

### 4\. 프롬프트 캐싱

매번 동일한 시스템 프롬프트나 예시 데이터를 API 요청에 포함시키는 것은 비효율적이며 비용을 증가시킨다. OpenAI의 **프롬프트 캐싱** 기능은 이러한 반복적인 프롬프트의 앞부분(prefix)을 캐시에 저장해두고, 다음 요청 시 재사용하여 **응답 속도를 높이고 비용을 절감**하는 아주 유용한 기능이다.

  * **동작 원리**: 1024 토큰 이상의 프롬프트에서, 동일한 접두사를 가진 요청이 오면 캐시된 결과를 사용한다.
  * **활용 팁**: 시스템 지침이나 변하지 않는 컨텍스트 정보처럼 **고정된 내용은 프롬프트의 앞부분**에, 사용자 질문과 같이 **가변적인 내용은 뒷부분**에 배치하는 것이 캐시 히트율을 높이는 핵심이다.

아래 예제는 긴 참조 문서를 프롬프트에 포함시키고, 질문만 바꿔가며 요청하는 상황이다.

```python
# 첫 번째 요청
with get_openai_callback() as cb:
    answer = llm.invoke(
        very_long_prompt.format("프롬프트 캐싱 기능에 대해 2문장으로 설명하세요")
    )
    print(cb) # 전체 토큰 비용 출력
    cached_tokens = answer.response_metadata["token_usage"]["prompt_tokens_details"]["cached_tokens"]
    print(f"캐싱된 토큰: {cached_tokens}") # 처음엔 0

# 두 번째 동일한 요청
with get_openai_callback() as cb:
    answer = llm.invoke(
        very_long_prompt.format("프롬프트 캐싱의 장점은 무엇인가요?")
    )
    print(cb) # 현저히 줄어든 비용 확인
    cached_tokens = answer.response_metadata["token_usage"]["prompt_tokens_details"]["cached_tokens"]
    print(f"캐싱된 토큰: {cached_tokens}") # 캐싱된 토큰 수 출력
```

두 번째 호출에서는 `Total Cost`가 크게 줄어들고 `캐싱된 토큰` 수가 증가한 것을 볼 수 있다. 이를 통해 상당한 비용 절감 효과를 얻을 수 있다.

-----

### 5\. 멀티모달

최신 모델들은 텍스트를 넘어 이미지까지 이해하는 **멀티모달(Multi-modal)** 기능을 지원한다. `gpt-4.1`과 같은 비전(Vision) 모델을 사용하면, 이미지에 대한 질문을 하고 설명을 생성하는 등 놀라운 작업들을 수행할 수 있다.

`langchain_teddynote`의 `MultiModal` 클래스를 사용하면 이미지 URL이나 로컬 파일 경로를 전달하는 것만으로 간단하게 이 기능을 사용할 수 있다.

#### **시스템 프롬프트 커스터마이징**

여기에 더해, 시스템 프롬프트를 지정하여 모델의 역할을 부여할 수도 있다. 예를 들어, 재무제표 이미지를 분석하는 '금융 AI 어시스턴트' 역할을 부여해 본다.

```python
from langchain_teddynote.models import MultiModal

# 시스템/유저 프롬프트 정의
system_prompt = """당신은 표(재무제표) 를 해석하는 금융 AI 어시스턴트다.
당신의 임무는 주어진 테이블 형식의 재무제표를 바탕으로 흥미로운 사실을 정리하여 친절하게 답변하는 것이다."""
user_prompt = "당신에게 주어진 표는 회사의 재무제표다. 흥미로운 사실을 정리하여 답변하라."

# 프롬프트를 포함한 멀티모달 객체 생성
multimodal_llm_with_prompt = MultiModal(
    llm, system_prompt=system_prompt, user_prompt=user_prompt
)

# 재무제표 이미지 URL
IMAGE_URL = "https://storage.googleapis.com/static.fastcampus.co.kr/prod/uploads/202212/080345-661/kwon-01.png"

# 이미지를 분석하고 스트리밍으로 답변 받기
answer = multimodal_llm_with_prompt.stream(IMAGE_URL)
stream_response(answer)
```
