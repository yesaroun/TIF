# Ch03 LLM 기본 용어

## 02 토큰

token(토큰)은 자연어 처리에서 텍스트를 처리학 위해 나눈 작은 단위(단어, 서브워드, 문자 등)  
텍스트를 토큰으로 나누는 과정을 tokenize(토큰화)  

### 토큰화 방식

- **문자 기반 토큰화** : 텍스트를 개별 문자로 나눠 각 문자를 하나의 토큰으로 취급
    - e.g. 'hello' -> h, e, l, l, o -> 5개 토큰 생성
- **단어 기반 토큰화** : 텍스트를 단어 단위로 나눔
    - e.g. 'hello, world!' -> 'hello', ',', 'world', '!' -> 4개

**이러한 차이는 LLM 텍스트 생성에 큰 영향**  
LLM은 먼저 나올 토큰을 생성하면 그 다움 나올 토큰을 확률적으로 계산해 생성하는 로직 -> 토큰이 많으면 틀린 토큰을 생성할 가능성이 높다.  
그래서 문자 기반 토큰화는 예측량 증가 -> 비용 증가 + 틀릴 확률 증가  
반면에 단어 기반 토큰화는 모든 단어를 저장한 '단어 사전'을 만들어야 하는데 너무 많아서 비효율적  

이를 보완하기 위해 단어 전체를 토큰화하는 대신 서브워드를 활용하는 **서브워드 기반 토큰화** 존재 -> **바이트 페어 인코딩**(BPE: Byte Pair Encoding): 단어를 처음에는 문자 단위로 분해한 후, 자주 등장하는 문자 쌍을 결합해 더 큰 서브워드로 만드는 방식  
*서브워드(subword): 접두사, 접미사, 어근 등이 해당*  

이렇게 토큰 단위로 나눈 후 숫자로 변환하는 과정을 거친다.  
이 숫자 변환은 좌표계 형식으로 표현, 이를 **임베딩 벡터**(embedding vector)  
임베딩 벡터는 글자 수나 단어 단위가 아니라 토큰 단위로 이뤄진다.  

### 토큰 사용량 계산

[Tiktokenizer](https://tiktokenizer.vercel.app/)
